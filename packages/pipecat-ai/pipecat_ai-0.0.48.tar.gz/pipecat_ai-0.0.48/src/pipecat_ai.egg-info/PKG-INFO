Metadata-Version: 2.1
Name: pipecat-ai
Version: 0.0.48
Summary: An open source framework for voice (and multimodal) assistants
License: BSD 2-Clause License
Project-URL: Source, https://github.com/pipecat-ai/pipecat
Project-URL: Website, https://pipecat.ai
Keywords: webrtc,audio,video,ai
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: BSD License
Classifier: Topic :: Communications :: Conferencing
Classifier: Topic :: Multimedia :: Sound/Audio
Classifier: Topic :: Multimedia :: Video
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp~=3.10.3
Requires-Dist: loguru~=0.7.2
Requires-Dist: Markdown~=3.7
Requires-Dist: numpy~=1.26.4
Requires-Dist: Pillow~=10.4.0
Requires-Dist: protobuf~=4.25.4
Requires-Dist: pydantic~=2.8.2
Requires-Dist: pyloudnorm~=0.1.1
Requires-Dist: resampy~=0.4.3
Provides-Extra: anthropic
Requires-Dist: anthropic~=0.34.0; extra == "anthropic"
Provides-Extra: assemblyai
Requires-Dist: assemblyai~=0.34.0; extra == "assemblyai"
Provides-Extra: aws
Requires-Dist: boto3~=1.35.27; extra == "aws"
Provides-Extra: azure
Requires-Dist: azure-cognitiveservices-speech~=1.40.0; extra == "azure"
Provides-Extra: canonical
Requires-Dist: aiofiles~=24.1.0; extra == "canonical"
Provides-Extra: cartesia
Requires-Dist: cartesia~=1.0.13; extra == "cartesia"
Requires-Dist: websockets~=13.1; extra == "cartesia"
Provides-Extra: daily
Requires-Dist: daily-python~=0.12.0; extra == "daily"
Provides-Extra: deepgram
Requires-Dist: deepgram-sdk~=3.7.3; extra == "deepgram"
Provides-Extra: elevenlabs
Requires-Dist: websockets~=13.1; extra == "elevenlabs"
Provides-Extra: examples
Requires-Dist: python-dotenv~=1.0.1; extra == "examples"
Requires-Dist: flask~=3.0.3; extra == "examples"
Requires-Dist: flask_cors~=4.0.1; extra == "examples"
Provides-Extra: fal
Requires-Dist: fal-client~=0.4.1; extra == "fal"
Provides-Extra: gladia
Requires-Dist: websockets~=13.1; extra == "gladia"
Provides-Extra: google
Requires-Dist: google-generativeai~=0.8.3; extra == "google"
Requires-Dist: google-cloud-texttospeech~=2.17.2; extra == "google"
Provides-Extra: gstreamer
Requires-Dist: pygobject~=3.48.2; extra == "gstreamer"
Provides-Extra: fireworks
Requires-Dist: openai~=1.37.2; extra == "fireworks"
Provides-Extra: krisp
Requires-Dist: pipecat-ai-krisp~=0.2.0; extra == "krisp"
Provides-Extra: langchain
Requires-Dist: langchain~=0.2.14; extra == "langchain"
Requires-Dist: langchain-community~=0.2.12; extra == "langchain"
Requires-Dist: langchain-openai~=0.1.20; extra == "langchain"
Provides-Extra: livekit
Requires-Dist: livekit~=0.17.5; extra == "livekit"
Requires-Dist: livekit-api~=0.7.1; extra == "livekit"
Requires-Dist: tenacity~=8.5.0; extra == "livekit"
Provides-Extra: lmnt
Requires-Dist: lmnt~=1.1.4; extra == "lmnt"
Provides-Extra: local
Requires-Dist: pyaudio~=0.2.14; extra == "local"
Provides-Extra: moondream
Requires-Dist: einops~=0.8.0; extra == "moondream"
Requires-Dist: timm~=1.0.8; extra == "moondream"
Requires-Dist: transformers~=4.44.0; extra == "moondream"
Provides-Extra: noisereduce
Requires-Dist: noisereduce~=3.0.3; extra == "noisereduce"
Provides-Extra: openai
Requires-Dist: openai~=1.50.2; extra == "openai"
Requires-Dist: websockets~=13.1; extra == "openai"
Requires-Dist: python-deepcompare~=1.0.1; extra == "openai"
Provides-Extra: openpipe
Requires-Dist: openpipe~=4.24.0; extra == "openpipe"
Provides-Extra: playht
Requires-Dist: pyht~=0.1.4; extra == "playht"
Requires-Dist: websockets~=13.1; extra == "playht"
Provides-Extra: silero
Requires-Dist: onnxruntime~=1.19.2; extra == "silero"
Provides-Extra: soundfile
Requires-Dist: soundfile~=0.12.1; extra == "soundfile"
Provides-Extra: together
Requires-Dist: openai~=1.50.2; extra == "together"
Provides-Extra: websocket
Requires-Dist: websockets~=13.1; extra == "websocket"
Requires-Dist: fastapi~=0.115.0; extra == "websocket"
Provides-Extra: whisper
Requires-Dist: faster-whisper~=1.0.3; extra == "whisper"

<div align="center">
¬†<img alt="pipecat" width="300px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png">
</div>

# Pipecat

[![PyPI](https://img.shields.io/pypi/v/pipecat-ai)](https://pypi.org/project/pipecat-ai) [![Discord](https://img.shields.io/discord/1239284677165056021)](https://discord.gg/pipecat) <a href="https://app.commanddash.io/agent/github_pipecat-ai_pipecat"><img src="https://img.shields.io/badge/AI-Code%20Agent-EB9FDA"></a>

`pipecat` is a framework for building voice (and multimodal) conversational agents. Things like personal coaches, meeting assistants, [story-telling toys for kids](https://storytelling-chatbot.fly.dev/), customer support bots, [intake flows](https://www.youtube.com/watch?v=lDevgsp9vn0), and snarky social companions.

Take a look at some example apps:

<p float="left">
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot"><img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/simple-chatbot/image.png" width="280" /></a>&nbsp;
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/storytelling-chatbot"><img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/storytelling-chatbot/image.png" width="280" /></a>
    <br/>
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/translation-chatbot"><img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/translation-chatbot/image.png" width="280" /></a>&nbsp;
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/moondream-chatbot"><img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/moondream-chatbot/image.png" width="280" /></a>
</p>

## Getting started with voice agents

You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you‚Äôre ready. You can also add a üìû telephone number, üñºÔ∏è image output, üì∫ video input, use different LLMs, and more.

```shell
# install the module
pip install pipecat-ai

# set up an .env file with API keys
cp dot-env.template .env
```

By default, in order to minimize dependencies, only the basic framework functionality is available. Some third-party AI services require additional dependencies that you can install with:

```shell
pip install "pipecat-ai[option,...]"
```

Your project may or may not need these, so they're made available as optional requirements. Here is a list:

- **AI services**: `anthropic`, `assemblyai`, `aws`, `azure`, `deepgram`, `gladia`, `google`, `fal`, `lmnt`, `moondream`, `openai`, `openpipe`, `playht`, `silero`, `whisper`, `xtts`
- **Transports**: `local`, `websocket`, `daily`

## Code examples

- [foundational](https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational) ‚Äî small snippets that build on each other, introducing one or two concepts at a time
- [example apps](https://github.com/pipecat-ai/pipecat/tree/main/examples/) ‚Äî complete applications that you can use as starting points for development

## A simple voice agent running locally

Here is a very basic Pipecat bot that greets a user when they join a real-time session. We'll use [Daily](https://daily.co) for real-time media transport, and [Cartesia](https://cartesia.ai/) for text-to-speech.

```python
import asyncio

from pipecat.frames.frames import EndFrame, TextFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineTask
from pipecat.pipeline.runner import PipelineRunner
from pipecat.services.cartesia import CartesiaTTSService
from pipecat.transports.services.daily import DailyParams, DailyTransport

async def main():
  # Use Daily as a real-time media transport (WebRTC)
  transport = DailyTransport(
    room_url=...,
    token="", # leave empty. Note: token is _not_ your api key
    bot_name="Bot Name",
    params=DailyParams(audio_out_enabled=True))

  # Use Cartesia for Text-to-Speech
  tts = CartesiaTTSService(
    api_key=...,
    voice_id=...
  )

  # Simple pipeline that will process text to speech and output the result
  pipeline = Pipeline([tts, transport.output()])

  # Create Pipecat processor that can run one or more pipelines tasks
  runner = PipelineRunner()

  # Assign the task callable to run the pipeline
  task = PipelineTask(pipeline)

  # Register an event handler to play audio when a
  # participant joins the transport WebRTC session
  @transport.event_handler("on_first_participant_joined")
  async def on_first_participant_joined(transport, participant):
    participant_name = participant.get("info", {}).get("userName", "")
    # Queue a TextFrame that will get spoken by the TTS service (Cartesia)
    await task.queue_frame(TextFrame(f"Hello there, {participant_name}!"))

  # Register an event handler to exit the application when the user leaves.
  @transport.event_handler("on_participant_left")
  async def on_participant_left(transport, participant, reason):
    await task.queue_frame(EndFrame())

  # Run the pipeline task
  await runner.run(task)

if __name__ == "__main__":
  asyncio.run(main())
```

Run it with:

```shell
python app.py
```

Daily provides a prebuilt WebRTC user interface. Whilst the app is running, you can visit at `https://<yourdomain>.daily.co/<room_url>` and listen to the bot say hello!

## WebRTC for production use

WebSockets are fine for server-to-server communication or for initial development. But for production use, you‚Äôll need client-server audio to use a protocol designed for real-time media transport. (For an explanation of the difference between WebSockets and WebRTC, see [this post.](https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/#webrtc))

One way to get up and running quickly with WebRTC is to sign up for a Daily developer account. Daily gives you SDKs and global infrastructure for audio (and video) routing. Every account gets 10,000 audio/video/transcription minutes free each month.

Sign up [here](https://dashboard.daily.co/u/signup) and [create a room](https://docs.daily.co/reference/rest-api/rooms) in the developer Dashboard.

## What is VAD?

Voice Activity Detection &mdash; very important for knowing when a user has finished speaking to your bot. If you are not using press-to-talk, and want Pipecat to detect when the user has finished talking, VAD is an essential component for a natural feeling conversation.

Pipecat makes use of WebRTC VAD by default when using a WebRTC transport layer. Optionally, you can use Silero VAD for improved accuracy at the cost of higher CPU usage.

```shell
pip install pipecat-ai[silero]
```

## Running the Krisp Audio Filter

To use the Krisp Filter in this project, you‚Äôll need access to the **Krisp C++ SDK**.

### Step 1: Obtain Access to the Krisp SDK
1. **Create a Krisp Account**: If you don‚Äôt already have an account, [sign up at Krisp](https://krisp.ai/) to access the SDK.
2. **Download the SDK**: Once you have an account, follow the instructions on the Krisp platform to download the [Krisp's desktop SDKs](https://sdk.krisp.ai/sdk/desktop).
3. **Export the path to you krisp SDK**:
`export KRISP_SDK_PATH=/PATH/TO/KRISP/SDK`

### Step 2: Install the `pipecat-krisp` Module
Once the environment variable `KRISP_SDK_PATH` is exported, activate your Python virtual environment and install it with `pip`:

```shell
source venv/bin/activate
pip install pipecat-ai[krisp]
```

## Hacking on the framework itself

_Note that you may need to set up a virtual environment before following the instructions below. For instance, you might need to run the following from the root of the repo:_

```shell
python3 -m venv venv
source venv/bin/activate
```

From the root of this repo, run the following:

```shell
pip install -r dev-requirements.txt
python -m build
```

This builds the package. To use the package locally (e.g. to run sample files), run

```shell
pip install --editable ".[option,...]"
```

If you want to use this package from another directory, you can run:

```shell
pip install "path_to_this_repo[option,...]"
```

### Running tests

From the root directory, run:

```shell
pytest --doctest-modules --ignore-glob="*to_be_updated*" --ignore-glob=*pipeline_source* src tests
```

## Setting up your editor

This project uses strict [PEP 8](https://peps.python.org/pep-0008/) formatting via [Ruff](https://github.com/astral-sh/ruff).

### Emacs

You can use [use-package](https://github.com/jwiegley/use-package) to install [emacs-lazy-ruff](https://github.com/christophermadsen/emacs-lazy-ruff) package and configure `ruff` arguments:

```elisp
(use-package lazy-ruff
  :ensure t
  :hook ((python-mode . lazy-ruff-mode))
  :config
  (setq lazy-ruff-format-command "ruff format")
  (setq lazy-ruff-only-format-block t)
  (setq lazy-ruff-only-format-region t)
  (setq lazy-ruff-only-format-buffer t))
```

`ruff` was installed in the `venv` environment described before, so you should be able to use [pyvenv-auto](https://github.com/ryotaro612/pyvenv-auto) to automatically load that environment inside Emacs.

```elisp
(use-package pyvenv-auto
  :ensure t
  :defer t
  :hook ((python-mode . pyvenv-auto-run)))

```

### Visual Studio Code

Install the
[Ruff](https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff) extension. Then edit the user settings (_Ctrl-Shift-P_ `Open User Settings (JSON)`) and set it as the default Python formatter, and enable formatting on save:

```json
"[python]": {
    "editor.defaultFormatter": "charliermarsh.ruff",
    "editor.formatOnSave": true
}
```

## Getting help

‚û°Ô∏è [Join our Discord](https://discord.gg/pipecat)

‚û°Ô∏è [Reach us on X](https://x.com/pipecat_ai)
