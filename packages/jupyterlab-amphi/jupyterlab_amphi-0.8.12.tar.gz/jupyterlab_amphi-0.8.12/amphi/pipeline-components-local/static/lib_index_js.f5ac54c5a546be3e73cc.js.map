{"version":3,"file":"lib_index_js.f5ac54c5a546be3e73cc.js","mappings":";;;;;;;;;;;;;;;;;;;;AAAiN;AACjJ;AAC+B;AACvC;AACxD,gCAAgC,qFAAiB;AACjD;AACA;AACA,8BAA8B,sFAAsF;AACpH,oBAAoB,wCAAwC,EAAE,uDAAY;AAC1E,0BAA0B,sDAAW;AACrC,+BAA+B,kDAAW;AAC1C,iCAAiC,UAAU,IAAI,GAAG;AAClD,aAAa;AACb,iCAAiC,sFAAkB;AACnD,gCAAgC,mDAAQ;AACxC;AACA;AACA;AACA,aAAa;AACb,oBAAoB,uBAAuB,EAAE,mDAAQ;AACrD;AACA,gCAAgC;AAChC,kCAAkC,0DAAmB,CAAC,4EAAY;AAClE;AACA,wBAAwB,6CAAM;AAC9B,0BAA0B,+CAAQ;AAClC;AACA,aAAa;AACb,iCAAiC,kDAAW;AAC5C,gBAAgB,4EAAQ,GAAG,gDAAgD;AAC3E,aAAa;AACb;AACA,+BAA+B,mDAAQ,cAAc,QAAQ,iGAAiG;AAC9J;AACA;AACA,6BAA6B,+EAAe;AAC5C;AACA,wBAAwB,4BAA4B,EAAE,6EAAa;AACnE,yEAAyE,kCAAkC;AAC3G;AACA,4BAA4B,iBAAiB;AAC7C;AACA,iBAAiB;AACjB;AACA;AACA,4BAA4B,iBAAiB;AAC7C;AACA,gFAAgF,OAAO;AACvF,iBAAiB;AACjB;AACA;AACA,8CAA8C,+CAAQ;AACtD;AACA,oBAAoB,0DAAmB,CAAC,uDAAc;AACtD,gBAAgB,qFAAiB;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB,gDAAgD,0DAAmB,CAAC,kDAAW,IAAI,2BAA2B,+CAAQ,SAAS;AAC/H,oBAAoB,0DAAmB,aAAa,mCAAmC;AACvF,wBAAwB,0DAAmB,CAAC,gDAAY;AACxD,4HAA4H,0DAAmB,aAAa,6EAA6E,2FAA2F;AACpU,wBAAwB,0DAAmB,CAAC,kDAAc;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kCAAkC,qJAAqJ;AACvL,mCAAmC,kDAAW;AAC9C,QAAQ,oFAAgB,GAAG,wCAAwC;AACnE,KAAK;AACL,IAAI,gDAAS;AACb;AACA,KAAK;AACL,YAAY,0DAAmB,CAAC,uDAAc;AAC9C,QAAQ,0DAAmB,CAAC,uFAAuB,IAAI,0OAA0O;AACjS;AAC6B;;;;;;;;;;;;;;;;;ACjHY;AACgB;AAClD,2BAA2B,iEAAiB;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B,4CAA4C;AACtE,0BAA0B,4CAA4C;AACtE,0BAA0B,wCAAwC;AAClE,0BAA0B,wCAAwC;AAClE,0BAA0B,8CAA8C;AACxE,0BAA0B,0CAA0C;AACpE,0BAA0B,wDAAwD;AAClF,0BAA0B,sDAAsD;AAChF,0BAA0B;AAC1B;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+GAA+G,8CAAU;AACzH;AACA,0BAA0B,QAAQ;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gCAAgC,IAAI,GAAG,SAAS;AAChD,sBAAsB,YAAY,GAAG,cAAc;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,+BAA+B;AAC3D;AACA;AACA;AACA,oHAAoH,WAAW;AAC/H;AACA;AACA;AACA,EAAE,WAAW,yBAAyB,sBAAsB;AAC5D,EAAE,WAAW,kBAAkB,cAAc;AAC7C,EAAE,WAAW,YAAY,aAAa;;AAEtC;AACA,EAAE,UAAU,IAAI,cAAc,OAAO,UAAU;AAC/C;AACA,MAAM,yBAAyB;AAC/B,IAAI,WAAW;AACf,IAAI,WAAW;AACf,IAAI,WAAW;AACf;AACA,EAAE,YAAY,IAAI,YAAY;AAC9B;AACA;AACA;AACA;;;;;;;;;;;;;;;;;ACzHyC;AACgB;AAClD,2BAA2B,iEAAiB;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,0BAA0B,gDAAgD;AAC1E,0BAA0B,wDAAwD;AAClF,0BAA0B,4CAA4C;AACtE,0BAA0B;AAC1B;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+GAA+G,8CAAU;AACzH;AACA,0BAA0B,QAAQ;AAClC;AACA;AACA;AACA;AACA,qBAAqB,QAAQ;AAC7B;AACA;AACA,uBAAuB,QAAQ;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gCAAgC,IAAI,GAAG,SAAS;AAChD,sBAAsB,YAAY,GAAG,cAAc;AACnD;AACA;AACA;AACA,OAAO,2CAA2C;AAClD,OAAO,0CAA0C;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,+BAA+B;AAC3D;AACA;AACA;AACA,kHAAkH,WAAW;AAC7H;AACA;AACA;AACA,EAAE,WAAW;AACb;AACA,aAAa,aAAa;AAC1B;;AAEA;AACA,EAAE,WAAW,kBAAkB,cAAc;AAC7C,EAAE,WAAW,oBAAoB,oBAAoB;AACrD,EAAE,WAAW,YAAY,aAAa;AACtC,EAAE,WAAW,gBAAgB,kBAAkB;AAC/C,EAAE,WAAW,iBAAiB,qBAAqB;;AAEnD;AACA,EAAE,UAAU,IAAI,cAAc,OAAO,UAAU;AAC/C;AACA,MAAM,yBAAyB;AAC/B,IAAI,WAAW;AACf,IAAI,WAAW;AACf,IAAI,WAAW;AACf,IAAI,WAAW;AACf,IAAI,WAAW;AACf,IAAI,WAAW;AACf;AACA,EAAE,YAAY,IAAI,YAAY;AAC9B;AACA;AACA;AACA;;;;;;;;;;;;;;;;;AClKsC;AACmB;AAClD,uBAAuB,iEAAiB;AAC/C;AACA,gCAAgC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6FAA6F,2CAAO;AACpG;AACA,0BAA0B,QAAQ;AAClC;AACA;AACA;AACA;AACA,qBAAqB,QAAQ;AAC7B;AACA;AACA,4BAA4B,+BAA+B;AAC3D;AACA;AACA;AACA;AACA;AACA,EAAE,YAAY,oBAAoB,6CAA6C;AAC/E;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACvCoD;AACK;AACF;AACF;AACI;AACI;AACY;AACZ;AACC;AACT;AACK;AACJ;AACO;AACP;AACO;AACG;AACK;AAC9D,oBAAoB,8DAAO;AAClC;AACA,YAAY,oDAAa;AACzB,CAAC;AACM,qBAAqB,8DAAO;AACnC;AACA,YAAY,0DAAc;AAC1B,CAAC;AACM,sBAAsB,8DAAO;AACpC;AACA,YAAY,yDAAe;AAC3B,CAAC;AACM,oBAAoB,8DAAO;AAClC;AACA,YAAY,oDAAa;AACzB,CAAC;AACM,uBAAuB,8DAAO;AACrC;AACA,YAAY,oDAAgB;AAC5B,CAAC;AACM,uBAAuB,8DAAO;AACrC;AACA,YAAY,oDAAY;AACxB,CAAC;AACM,yBAAyB,8DAAO;AACvC;AACA,YAAY,yDAAkB;AAC9B,CAAC;AACM,yBAAyB,8DAAO;AACvC;AACA,YAAY,sDAAkB;AAC9B,CAAC;AACM,6BAA6B,8DAAO;AAC3C;AACA,YAAY,8DAAsB;AAClC,CAAC;AACM,yBAAyB,8DAAO;AACvC;AACA,YAAY,4DAAc;AAC1B,CAAC;AACM,yBAAyB,8DAAO;AACvC;AACA,YAAY,wDAAc;AAC1B,CAAC;AACM,uBAAuB,8DAAO;AACrC;AACA,YAAY,qDAAY;AACxB,CAAC;AACM,2BAA2B,8DAAO;AACzC;AACA,YAAY,6DAAoB;AAChC,CAAC;AACM,sBAAsB,8DAAO;AACpC;AACA,YAAY,oDAAe;AAC3B,CAAC;AACM,uBAAuB,8DAAO;AACrC;AACA,YAAY,yDAAgB;AAC5B,CAAC;AACM,uBAAuB,8DAAO;AACrC;AACA,YAAY,qDAAgB;AAC5B,CAAC;;;;;;;;;;;;;;;;;;;;AChFqE;AACF;AACpE;AACA;AACA;AACA;AACA,eAAe,gFAAgB;AAC/B;AACA;AACA;AACA,sCAAsC,iDAAQ;AAC9C,sCAAsC,qDAAY;AAClD,sCAAsC,qDAAY;AAClD;AACA;AACA,iEAAe,MAAM,EAAC","sources":["webpack://@amphi/pipeline-components-local/./lib/components/BaseCoreComponent.js","webpack://@amphi/pipeline-components-local/./lib/components/transforms/OllamaLookUp.js","webpack://@amphi/pipeline-components-local/./lib/components/transforms/OpenAILookUp.js","webpack://@amphi/pipeline-components-local/./lib/components/transforms/SqlQuery.js","webpack://@amphi/pipeline-components-local/./lib/icons.js","webpack://@amphi/pipeline-components-local/./lib/index.js"],"sourcesContent":["import { CodeGenerator, PipelineService, PipelineComponent, createZoomSelector, GenerateUIFormComponent, onChange, renderComponentUI, renderHandle, setDefaultConfig } from '@amphi/pipeline-components-manager';\nimport React, { useCallback, useEffect, useState } from 'react';\nimport { Handle, NodeToolbar, Position, useReactFlow, useStore, useStoreApi } from 'reactflow';\nimport { playCircleIcon, settingsIcon } from '../icons';\nclass BaseCoreComponent extends PipelineComponent() {\n    constructor(name, id, description, type, fileDrop, category, icon, defaultConfig, form) {\n        super();\n        this.UIComponent = ({ id, data, context, componentService, manager, commands, rendermimeRegistry, settings }) => {\n            const { setNodes, deleteElements, setViewport } = useReactFlow();\n            const store = useStoreApi();\n            const deleteNode = useCallback(() => {\n                deleteElements({ nodes: [{ id }] });\n            }, [id, deleteElements]);\n            const zoomSelector = createZoomSelector();\n            const showContent = useStore(zoomSelector);\n            const selector = (s) => ({\n                nodeInternals: s.nodeInternals,\n                edges: s.edges,\n            });\n            const { nodeInternals, edges } = useStore(selector);\n            const nodeId = id;\n            const internals = { nodeInternals, edges, nodeId, componentService };\n            const handleElement = React.createElement(renderHandle, {\n                type: this._type,\n                Handle: Handle,\n                Position: Position,\n                internals: internals\n            });\n            const handleChange = useCallback((evtTargetValue, field) => {\n                onChange({ evtTargetValue, field, nodeId, store, setNodes });\n            }, [nodeId, store, setNodes]);\n            // Selector to determine if the node is selected\n            const isSelected = useStore((state) => { var _a; return !!((_a = state.nodeInternals.get(id)) === null || _a === void 0 ? void 0 : _a.selected); });\n            const executeUntilComponent = () => {\n                const timestamp = Date.now();\n                const flow = PipelineService.filterPipeline(context.model.toString());\n                // Get nodes to traverse and related data\n                const { nodesToTraverse, nodesMap } = CodeGenerator.computeNodesToTraverse(flow, nodeId, componentService);\n                commands.execute('pipeline-editor:run-pipeline-until', { nodeId: nodeId, context: context }).then(result => {\n                    setNodes(prevNodes => prevNodes.map(node => nodesToTraverse.includes(node.id)\n                        ? { ...node, data: { ...node.data, lastExecuted: timestamp, successfulExecution: true } }\n                        : node));\n                })\n                    .catch(reason => {\n                    setNodes(prevNodes => prevNodes.map(node => nodesToTraverse.includes(node.id)\n                        ? { ...node, data: { ...node.data, successfulExecution: null } }\n                        : node));\n                    console.error(`Error with pipeline, nodes not updated.'.\\n${reason}`);\n                });\n                ;\n            };\n            const [modalOpen, setModalOpen] = useState(false);\n            let enableExecution = settings.get('enableExecution').composite;\n            return (React.createElement(React.Fragment, null,\n                renderComponentUI({\n                    id: id,\n                    data: data,\n                    context: context,\n                    manager: manager,\n                    commands: commands,\n                    name: this._name,\n                    ConfigForm: BaseCoreComponent.ConfigForm({\n                        nodeId: id,\n                        data,\n                        context,\n                        componentService,\n                        manager,\n                        commands,\n                        store,\n                        setNodes,\n                        type: this._type,\n                        name: this._name,\n                        defaultConfig: this._default,\n                        form: this._form,\n                        handleChange,\n                        modalOpen,\n                        setModalOpen\n                    }),\n                    Icon: this._icon,\n                    showContent: showContent,\n                    handle: handleElement,\n                    deleteNode: deleteNode,\n                    setViewport: setViewport,\n                    handleChange,\n                    isSelected\n                }),\n                (showContent || isSelected) && (React.createElement(NodeToolbar, { isVisible: true, position: Position.Bottom },\n                    React.createElement(\"button\", { onClick: () => setModalOpen(true) },\n                        React.createElement(settingsIcon.react, null)),\n                    (this._type.includes('input') || this._type.includes('processor') || this._type.includes('output')) && (React.createElement(\"button\", { onClick: () => executeUntilComponent(), disabled: !enableExecution, style: { opacity: enableExecution ? 1 : 0.5, cursor: enableExecution ? 'pointer' : 'not-allowed' } },\n                        React.createElement(playCircleIcon.react, null)))))));\n        };\n        this._name = name;\n        this._id = id;\n        this._description = description;\n        this._type = type;\n        this._fileDrop = fileDrop;\n        this._category = category;\n        this._icon = icon;\n        this._default = defaultConfig;\n        this._form = form;\n    }\n}\nBaseCoreComponent.ConfigForm = ({ nodeId, data, context, componentService, manager, commands, store, setNodes, type, name, defaultConfig, form, handleChange, modalOpen, setModalOpen }) => {\n    const handleSetDefaultConfig = useCallback(() => {\n        setDefaultConfig({ nodeId, store, setNodes, defaultConfig });\n    }, [nodeId, store, setNodes, defaultConfig]);\n    useEffect(() => {\n        handleSetDefaultConfig();\n    }, [handleSetDefaultConfig]);\n    return (React.createElement(React.Fragment, null,\n        React.createElement(GenerateUIFormComponent, { nodeId: nodeId, type: type, name: name, form: form, data: data, context: context, componentService: componentService, manager: manager, commands: commands, handleChange: handleChange, modalOpen: modalOpen, setModalOpen: setModalOpen })));\n};\nexport { BaseCoreComponent };\n","import { ollamaIcon } from '../../icons';\nimport { BaseCoreComponent } from '../BaseCoreComponent';\nexport class OllamaLookUp extends BaseCoreComponent {\n    constructor() {\n        const defaultConfig = {\n            maxToken: 256,\n            temperature: 0.3,\n            model: \"llama3.2:3b\",\n            ollamaEndpoint: \"http://localhost:11434\",\n            systemPrompt: \"You are part of a Python data pipeline using Pandas. Your task is to generate responses for each row of a DataFrame. Just provide the response as short as possible, don't write any sentence.\"\n        };\n        const form = {\n            idPrefix: \"component__form\",\n            fields: [\n                {\n                    type: \"textarea\",\n                    label: \"Prompt\",\n                    id: \"prompt\"\n                },\n                {\n                    type: \"columns\",\n                    label: \"Columns\",\n                    tooltip: \"Columns data to send with Prompt\",\n                    id: \"columns\"\n                },\n                {\n                    type: \"input\",\n                    label: \"Ollama Endpoint\",\n                    id: \"ollamaEndpoint\",\n                    placeholder: \"http://localhost:11434\",\n                    connection: \"Ollama\",\n                    advanced: true\n                },\n                {\n                    type: \"selectCustomizable\",\n                    label: \"Model\",\n                    id: \"model\",\n                    tooltip: \"Select an Ollama model. You must have access to it or pulled it locally. Check out: https://ollama.com/library\",\n                    options: [\n                        { value: \"llama3.2:1b\", label: \"llama3.2:1b\" },\n                        { value: \"llama3.2:3b\", label: \"llama3.2:3b\" },\n                        { value: \"gemma2:2b\", label: \"gemma2:2b\" },\n                        { value: \"gemma2:9b\", label: \"gemma2:9b\" },\n                        { value: \"qwen2.5:0.5b\", label: \"qwen2.5:0.5b\" },\n                        { value: \"qwen2.5:3b\", label: \"qwen2.5:3b\" },\n                        { value: \"mistral-small:22b\", label: \"mistral-small:22b\" },\n                        { value: \"mistral-nemo:12b\", label: \"mistral-nemo:12b\" },\n                        { value: \"phi3.5:3.8b\", label: \"phi3.5:3.8b\" }\n                    ],\n                    advanced: true\n                },\n                {\n                    type: \"input\",\n                    label: \"New column name\",\n                    id: \"newColumnName\",\n                    placeholder: \"Type new column name\",\n                    advanced: true\n                }\n            ],\n        };\n        const description = \"Use Ollama Lookup to prompt Ollama based on column values and create a new column with the response.\";\n        super(\"Ollama Prompt\", \"ollamaLookup\", description, \"pandas_df_processor\", [], \"transforms.AI Prompt\", ollamaIcon, defaultConfig, form);\n    }\n    provideDependencies({ config }) {\n        let deps = [];\n        deps.push('ollama');\n        return deps;\n    }\n    provideImports() {\n        return [\"import pandas as pd\", \"from ollama import Client\"];\n    }\n    provideFunctions() {\n        let functions = [];\n        const code = `\ndef generate_ollama_response(\n  row,  # The data row containing the values\n  column_data,  # The list of column names to include in the prompt\n  client, # Ollama client\n  user_prompt,  # The initial user prompt text\n  model  # The Ollama model to use\n):\n  # Create a dynamic prompt by including the user prompt and the corresponding data from the specified columns\n  column_values = ', '.join([f\"{col}: {row[col]}\" for col in column_data])  # Construct column-value pairs\n  dynamic_prompt = f\"{user_prompt}: {column_values}\"  # Combine user prompt with column values\n\n  # Call Ollama's API to generate the response\n  response = client.generate(\n      model=model,  # Llama model used for response generation\n      prompt=dynamic_prompt,  # Include system and user prompts\n  )\n\n  # Return the generated response's content\n  return response['response']\n`;\n        functions.push(code);\n        return functions;\n    }\n    generateComponentCode({ config, inputName, outputName }) {\n        // Extract the column names from the config object\n        const columnNames = config.columns.map((col) => col.value);\n        // Determine the column name to use\n        const newColumnName = config.newColumnName && config.newColumnName.trim() ? config.newColumnName : `llama_${outputName}`;\n        // Generate Python code to configure Ollama API and process data\n        const code = `\n# Ollama request parameters\n${outputName}_client = Client(host='${config.ollamaEndpoint}')\n${outputName}_user_prompt = \"${config.prompt}\"\n${outputName}_model = \"${config.model}\"  # Model to use for generating responses\n\n# Apply the function to generate output for each row\n${inputName}['${newColumnName}'] = ${inputName}.apply(lambda row: generate_ollama_response(\n  row, \n  [\"${columnNames.join('\", \"')}\"],\n  ${outputName}_client,\n  ${outputName}_user_prompt,\n  ${outputName}_model\n), axis=1)\n${outputName} = ${inputName}  # Set the modified DataFrame to the output variable\n`;\n        return code;\n    }\n}\n","import { openAiIcon } from '../../icons';\nimport { BaseCoreComponent } from '../BaseCoreComponent';\nexport class OpenAILookUp extends BaseCoreComponent {\n    constructor() {\n        const defaultConfig = {\n            maxToken: 256,\n            temperature: 0.3,\n            model: \"gpt-3.5-turbo\",\n            systemPrompt: \"You are part of a Python data pipeline using Pandas. Your task is to generate responses for each row of a DataFrame. Just provide the response as short as possible, don't write any sentence.\"\n        };\n        const form = {\n            idPrefix: \"component__form\",\n            fields: [\n                {\n                    type: \"textarea\",\n                    label: \"Prompt\",\n                    id: \"prompt\"\n                },\n                {\n                    type: \"columns\",\n                    label: \"Columns\",\n                    tooltip: \"Columns data to send with Prompt\",\n                    id: \"columns\"\n                },\n                {\n                    type: \"textarea\",\n                    label: \"System Prompt\",\n                    id: \"systemPrompt\",\n                    advanced: true\n                },\n                {\n                    type: \"input\",\n                    label: \"Open API Key\",\n                    id: \"openaiApiKey\",\n                    connection: \"OpenAI\",\n                    advanced: true\n                },\n                {\n                    type: \"selectCustomizable\",\n                    label: \"Model\",\n                    id: \"model\",\n                    options: [\n                        { value: \"gpt-3.5-turbo\", label: \"gpt-3.5-turbo\" },\n                        { value: \"gpt-3.5-turbo-16k\", label: \"gpt-3.5-turbo-16k\" },\n                        { value: \"gpt-4-turbo\", label: \"gpt-4-turbo\" },\n                        { value: \"gpt-4o\", label: \"gpt-4o\" }\n                    ],\n                    advanced: true\n                },\n                {\n                    type: \"boolean\",\n                    label: \"JSON Mode\",\n                    id: \"jsonMode\",\n                    advanced: true\n                },\n                {\n                    type: \"inputNumber\",\n                    label: \"Max Token\",\n                    id: \"maxToken\",\n                    min: 1,\n                    advanced: true\n                },\n                {\n                    type: \"inputNumber\",\n                    label: \"Temperature\",\n                    id: \"temperature\",\n                    min: 0,\n                    max: 1,\n                    advanced: true\n                },\n                {\n                    type: \"input\",\n                    label: \"New column name\",\n                    id: \"newColumnName\",\n                    placeholder: \"Type new column name\",\n                    advanced: true\n                }\n            ],\n        };\n        const description = \"Use OpenAI Lookup to prompt OpenAI based on column values and create a new column with the response.\";\n        super(\"OpenAI Prompt\", \"openAiLookup\", description, \"pandas_df_processor\", [], \"transforms.AI Prompt\", openAiIcon, defaultConfig, form);\n    }\n    provideDependencies({ config }) {\n        let deps = [];\n        deps.push('openai');\n        return deps;\n    }\n    provideImports({ config }) {\n        return [\"import pandas as pd\", \"from openai import OpenAI\"];\n    }\n    provideFunctions({ config }) {\n        let functions = [];\n        const code = `\ndef generate_gpt_response(\n  row,  # The data row containing the values\n  column_data,  # The list of column names to include in the prompt\n  client,  # OpenAI client for generating responses\n  user_prompt,  # The initial user prompt text\n  system_prompt,  # The system-level instruction\n  model,  # The OpenAI model to use\n  max_tokens,  # Maximum number of tokens for the response\n  temperature  # Controls the variability of the response\n):\n  # Create a dynamic prompt by including the user prompt and the corresponding data from the specified columns\n  column_values = ', '.join([f\"{col}: {row[col]}\" for col in column_data])  # Construct column-value pairs\n  dynamic_prompt = f\"{user_prompt}: {column_values}\"  # Combine user prompt with column values\n  \n  # Set up the messages for the OpenAI API request\n  messages = [\n      {\"role\": \"system\", \"content\": system_prompt},  # System-level context\n      {\"role\": \"user\", \"content\": dynamic_prompt},  # User prompt with dynamic content\n  ]\n  \n  # Call OpenAI's ChatCompletion endpoint to generate the response\n  response = client.chat.completions.create(\n      model=model,  # GPT model used for response generation\n      messages=messages,  # Include system and user prompts\n      max_tokens=max_tokens,  # Maximum response length\n      temperature=temperature  # Variability in response generation\n  )\n  \n  # Return the generated response's content\n  return response.choices[0].message.content\n`;\n        functions.push(code);\n        return functions;\n    }\n    generateComponentCode({ config, inputName, outputName }) {\n        // Extract the column names from the config object\n        const columnNames = config.columns.map((col) => col.value);\n        // Determine the column name to use\n        const newColumnName = config.newColumnName && config.newColumnName.trim() ? config.newColumnName : `gpt_${outputName}`;\n        // Generate Python code to configure OpenAI API and process data\n        const code = `\n# Set the OpenAI API key for authentication\n${outputName}_client = OpenAI(\n  # This is the default and can be omitted\n  api_key=\"${config.token}\"\n)\n\n# OpenAI request parameters\n${outputName}_user_prompt = \"${config.prompt}\"\n${outputName}_system_prompt = \"${config.systemPrompt}\"\n${outputName}_model = \"${config.model}\"  # Model to use for generating responses\n${outputName}_max_tokens = ${config.maxToken}  # Maximum number of tokens for the generated response\n${outputName}_temperature = ${config.temperature}  # Response variability based on the specified temperature\n\n# Apply the function to generate output for each row\n${inputName}['${newColumnName}'] = ${inputName}.apply(lambda row: generate_gpt_response(\n  row, \n  [\"${columnNames.join('\", \"')}\"], \n  ${outputName}_client,\n  ${outputName}_user_prompt,\n  ${outputName}_system_prompt, \n  ${outputName}_model, \n  ${outputName}_max_tokens, \n  ${outputName}_temperature\n), axis=1)\n${outputName} = ${inputName}  # Set the modified DataFrame to the output variable\n`;\n        return code;\n    }\n}\n","import { boxIcon } from '../../icons';\nimport { BaseCoreComponent } from '../BaseCoreComponent';\nexport class SQLQuery extends BaseCoreComponent {\n    constructor() {\n        const defaultConfig = { query: \"SELECT * FROM input_df1\" };\n        const form = {\n            idPrefix: \"component__form\",\n            fields: [\n                {\n                    type: \"codeTextarea\",\n                    label: \"SQL\",\n                    mode: \"sql\",\n                    id: \"query\",\n                    placeholder: \"Enter your SQL Query here. Table is named input_df1.\",\n                    advanced: true\n                }\n            ],\n        };\n        const description = \"Run a SQL query to select and update the dataset.\";\n        super(\"SQL Query\", \"sqlQuery\", description, \"pandas_df_processor\", [], \"transforms\", boxIcon, defaultConfig, form);\n    }\n    provideDependencies({ config }) {\n        let deps = [];\n        deps.push('duckdb');\n        return deps;\n    }\n    provideImports({ config }) {\n        return [\"import pandas as pd\", \"import duckdb\"];\n    }\n    generateComponentCode({ config, inputName, outputName }) {\n        // Escape triple quotes in the query to prevent syntax errors\n        const escapedQuery = config.query.replace(/\"\"\"/g, '\\\\\"\"\"');\n        // Template for the pandas query code using triple quotes for multi-line SQL queries\n        const code = `\n# Execute SQL Query using DuckDB\n${outputName} = duckdb.query(\"\"\"${escapedQuery.replace('input_df1', inputName)}\"\"\").to_df().convert_dtypes()\n`;\n        return code;\n    }\n}\n","import { LabIcon } from '@jupyterlab/ui-components';\nimport redditIconSvgStr from '../style/icons/reddit.svg';\nimport trinoIconSvgStr from '../style/icons/trino.svg';\nimport chromaSvgStr from '../style/icons/chroma.svg';\nimport ollamaIconSvgStr from '../style/icons/ollama.svg';\nimport pineconeIconSvgStr from '../style/icons/pinecone.svg';\nimport changeCircleIconSvgStr from '../style/icons/change-circle-24.svg';\nimport sortIconSvgStr from '../style/icons/sort-desc-24.svg';\nimport MarkdownSvgStr from '../style/icons/markdown-fill.svg';\nimport openAiSvgStr from '../style/icons/openai.svg';\nimport htmlLineSvgStr from '../style/icons/html-line.svg';\nimport apiIconSvgStr from '../style/icons/api-24.svg';\nimport splitIconSvgStr from '../style/icons/scissors-24.svg';\nimport boxIconSvgStr from '../style/icons/box-16.svg';\nimport engineIconSvgStr from '../style/icons/service-16.svg';\nimport settingsIconSvgStr from '../style/icons/settings-16.svg';\nimport playCircleIconSvgStr from '../style/icons/play-circle-16.svg';\nexport const apiIcon = new LabIcon({\n    name: 'amphi:api-icon',\n    svgstr: apiIconSvgStr\n});\nexport const sortIcon = new LabIcon({\n    name: 'amphi:sortIcon',\n    svgstr: sortIconSvgStr\n});\nexport const splitIcon = new LabIcon({\n    name: 'amphi:splitIcon',\n    svgstr: splitIconSvgStr\n});\nexport const boxIcon = new LabIcon({\n    name: 'amphi:box-icon',\n    svgstr: boxIconSvgStr\n});\nexport const redditIcon = new LabIcon({\n    name: 'amphi:redditIcon',\n    svgstr: redditIconSvgStr\n});\nexport const openAiIcon = new LabIcon({\n    name: 'amphi:openAiIcon',\n    svgstr: openAiSvgStr\n});\nexport const settingsIcon = new LabIcon({\n    name: 'amphi:settings-config-icon',\n    svgstr: settingsIconSvgStr\n});\nexport const pineconeIcon = new LabIcon({\n    name: 'amphi:pinecone-icon',\n    svgstr: pineconeIconSvgStr\n});\nexport const changeCircleIcon = new LabIcon({\n    name: 'amphi:changeCircle-icon',\n    svgstr: changeCircleIconSvgStr\n});\nexport const markdownIcon = new LabIcon({\n    name: 'amphi:markdown-icon',\n    svgstr: MarkdownSvgStr\n});\nexport const htmlLineIcon = new LabIcon({\n    name: 'amphi:htmlLine-icon',\n    svgstr: htmlLineSvgStr\n});\nexport const chromaIcon = new LabIcon({\n    name: 'amphi:chroma-icon',\n    svgstr: chromaSvgStr\n});\nexport const playCircleIcon = new LabIcon({\n    name: 'amphi:play-circle-icon',\n    svgstr: playCircleIconSvgStr\n});\nexport const trinoIcon = new LabIcon({\n    name: 'amphi:trino-icon',\n    svgstr: trinoIconSvgStr\n});\nexport const engineIcon = new LabIcon({\n    name: 'amphi:engine-icon',\n    svgstr: engineIconSvgStr\n});\nexport const ollamaIcon = new LabIcon({\n    name: 'amphi:ollama-icon',\n    svgstr: ollamaIconSvgStr\n});\n","import { ComponentManager } from \"@amphi/pipeline-components-manager\";\nimport { SQLQuery, OpenAILookUp, OllamaLookUp } from './components';\nconst plugin = {\n    id: '@amphi/pipeline-components-local',\n    description: 'Adds a step counter/button (1 of 3 related examples). This extension holds the UI/interface',\n    autoStart: true,\n    requires: [ComponentManager],\n    activate: (app, componentService) => {\n        console.log('JupyterLab extension pipeline-components-local is activated!');\n        // Processors\n        componentService.addComponent(SQLQuery.getInstance());\n        componentService.addComponent(OpenAILookUp.getInstance());\n        componentService.addComponent(OllamaLookUp.getInstance());\n    }\n};\nexport default plugin;\n"],"names":[],"sourceRoot":""}