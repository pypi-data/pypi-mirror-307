# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/05_deepLearningNewsvendor.ipynb.

# %% auto 0
__all__ = ['ACTIVATIONS', 'NewsvendorData', 'DeepLearningNewsvendor']

# %% ../../nbs/05_deepLearningNewsvendor.ipynb 4
from ._base import BaseNewsvendor, DataDrivenMixin
from ..utils.validation import check_cu_co
from ..metrics import average_costs

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import DataLoader, Dataset

from sklearn.utils.validation import check_is_fitted
import numpy as np

ACTIVATIONS = ['elu', 'selu', 'linear', 'tanh', 'relu', 'softmax', 'softsign', 'softplus',
               'sigmoid', 'hard_sigmoid', 'exponential']

# %% ../../nbs/05_deepLearningNewsvendor.ipynb 6
class NewsvendorData(Dataset):

    def __init__(self, x, y):
        # create torch tensors

        self.x=torch.from_numpy(x)
        self.y=torch.from_numpy(y)
        
        # convert to torch float32
        self.x=self.x.float()
        self.y=self.y.float()

        self.n_samples=y.shape[0]

    def __getitem__(self, index):
        
        x = self.x[index]
        y = self.y[index]

        return x, y

    def __len__(self):
        return self.n_samples

class DeepLearningNewsvendor(BaseNewsvendor, DataDrivenMixin):

    """A Deep-Learning model to solve the Newsvendor problem.

    This class implements the approach described in [1].

    Parameters
    ----------
    cu : {array-like of shape (n_outputs,), Number or None}, default=None
       The underage costs per unit. If None, then underage costs are one
       for each target variable
    co : {array-like of shape (n_outputs,), Number or None}, default=None
       The overage costs per unit. If None, then overage costs are one
       for each target variable
    neurons : list, default=[100,50]
        The ith element represents the number of neurons in the ith hidden layer
        Only used when hidden_layers='custom'.
    activations : list, default=['relu','relu']
        The ith element of the list represents the activation function of the ith layer.
        Valid activation functions are: 'elu', 'selu', 'linear', 'tanh', 'relu', 'softmax',
        'softsign', 'softplus','sigmoid', 'hard_sigmoid', 'exponential'.
        Only used when hidden_layers='custom'.
    optimizer: {'adam', 'sgd'}, default='adam'
        The optimizer to be used.
    epochs: int, default=100
        Number of epochs to train the model
    random_state: int, RandomState instance, default=None
        Pass an int for reproducible results across multiple function calls.
    verbose: int 0, 1, or 2, default=0
        Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.

    Attributes
    ----------
    model_ : torch.nn.Module
        The underlying model
    n_features_ : int
        The number of features when ``fit`` is performed.
    n_outputs_ : int
        The number of outputs.
    cu_ : ndarray, shape (n_outputs,)
        Validated underage costs.
    co_ : ndarray, shape (n_outputs,)
        Validated overage costs.

    References
    ----------
    .. [1] Afshin Oroojlooyjadid, Lawrence V. Snyder, Martin Takáˇc,
            "Applying Deep Learning to the Newsvendor Problem", 2018.
    """

    ACTIVATION_FUNCTIONS = {
        'ReLU': nn.ReLU,
        'Sigmoid': nn.Sigmoid,
        'Tanh': nn.Tanh,
        'LeakyReLU': nn.LeakyReLU,
        'ELU': nn.ELU,
        'SELU': nn.SELU,
        'GELU': nn.GELU
        }

    def __init__(self, cu=None, co=None, neurons=[100, 50], activations=['ReLU', 'ReLU'], optimizer='adam', epochs=100, batch_size=64, learning_rate=0.003, drop_prob=0.0,
                 random_state=None, verbose=1, print_freq=10):
        self.neurons = neurons
        self.activations = activations
        self.optimizer = optimizer
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.drop_prob = drop_prob
        self.random_state = random_state
        self.verbose = verbose
        self.print_freq = print_freq
        
        super().__init__(
            cu=cu,
            co=co)

    @staticmethod
    def max_or_zero(data):
        value = torch.max(data, torch.zeros_like(data))
        return value

    @staticmethod
    def pinball_loss(cu, co, demand, order_quantity):

        if len(demand.shape)==1:
            demand = demand.unsqueeze(1)
        
        assert demand.shape == order_quantity.shape

        cu = torch.tensor(cu, dtype=torch.float32)
        co = torch.tensor(co, dtype=torch.float32)

        cu = cu.to(demand.device)
        co = co.to(demand.device)

        underage=cu*DeepLearningNewsvendor.max_or_zero(demand-order_quantity)
        overage=co*DeepLearningNewsvendor.max_or_zero(order_quantity-demand)

        loss=underage+overage
    
        return loss

    def _create_model(self):

        neurons = self.neurons
        activations = self.activations
        n_features = self.n_features_
        n_outputs = self.n_outputs_

        if self.random_state is not None:
            torch.manual_seed(self.random_state)

        layers = []

        for i in range(len(neurons)):
            if i == 0:
                layers.append(nn.Linear(n_features, neurons[i]))
                layers.append(DeepLearningNewsvendor.ACTIVATION_FUNCTIONS[activations[i]]())
                if self.drop_prob > 0:
                    layers.append(nn.Dropout(self.drop_prob))
            else:
                layers.append(nn.Linear(neurons[i-1], neurons[i]))
                layers.append(DeepLearningNewsvendor.ACTIVATION_FUNCTIONS[activations[i]]())
                if self.drop_prob > 0:
                    layers.append(nn.Dropout(self.drop_prob))
            
        layers.append(nn.Linear(neurons[-1], n_outputs))

        model = nn.Sequential(*layers)

        return model

    def fit(self, X, y, validation_data=None):
            
            """Fit the model to the training set (X, y).

            Parameters
            ----------
            X : array-like of shape (n_samples, n_features)
                The training input samples.
            y : array-like of shape (n_samples, n_outputs)
                The target values.

            Returns
            ----------
            self : DeepLearningNewsvendor
                Fitted estimator
            """

            # Validate input parameters
            self._validate_hyperparameters()

            X, y = self._validate_data(X, y, multi_output=True)

            if y.ndim == 1:
                y = np.reshape(y, (-1, 1))

            # Determine output settings
            self.X_ = X
            self.y_ = y
            self.n_features_ = X.shape[1]
            self.n_outputs_ = y.shape[1]
        
            # Check and format under- and overage costs
            self.cu_, self.co_ = check_cu_co(self.cu, self.co, self.n_outputs_)

            # create model
            model = self._create_model()

            dataset_train=NewsvendorData(X, y)

            train_loader=DataLoader(dataset=dataset_train, batch_size=self.batch_size, shuffle=True)

            if self.optimizer == 'adam':
                self.optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)
            elif self.optimizer == 'sgd':
                self.optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate)
            else:
                raise ValueError("Invalid optimizer")
            
            for epoch in range(self.epochs):
                
                model.train()

                epoch_loss = 0
                for i, (output) in enumerate(train_loader):
                    
                    feat, labels = output
                    outputs=model(feat)

                    loss=self.pinball_loss(self.cu_, self.co_, labels, outputs).mean()

                    self.optimizer.zero_grad()

                    loss.backward()

                    self.optimizer.step()

                    epoch_loss += loss.item()

                model.eval()
                
                if (epoch+1) % self.print_freq == 0 and self.verbose > 0:

                    if validation_data is not None:

                        X_val, y_val = validation_data

                        X_val, y_val = self._validate_data(X_val, y_val, multi_output=True)

                        # get pytorch tensor
                        X_val = torch.from_numpy(X_val)
                        X_val = X_val.float()

                        # ensure shape
                        if X_val.ndim == 1:
                            X_val = X_val.unsqueeze(0)
                        y_val_hat = model(X_val)

                        y_val_hat=y_val_hat.detach().numpy().squeeze().tolist()
                        
                        cost = np.round(average_costs(y_val, y_val_hat, cu=self.cu, co=self.co),2)
                        
                        print(f"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss}, Validation Cost: {cost}")
                        
                    else:
                        print(f"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss}")

            self.model_ = model

            return self
            
            # return y_val, y_val_hat
          
    
    def predict(self, X):
    
        """Predict values for X.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The input samples to predict.

        Returns
        ----------
        y : array-like of shape (n_samples, n_outputs)
            The predicted values
        """

        self.model_.eval()
        
        X = self._validate_data(X)

        dataset_test=NewsvendorData(X, np.zeros((X.shape[0], self.n_outputs_)))

        test_loader=DataLoader(dataset=dataset_test, batch_size=self.batch_size, shuffle=False)

        pred = []
        for i, (output) in enumerate(test_loader):
            
            feat, _ = output
            outputs=self.model_(feat)
            outputs=outputs.detach().numpy().squeeze()
            pred.extend(outputs.tolist())
        
        return pred

    def _validate_hyperparameters(self):
        
        """validate hyperparameters"""

        # Make sure self.neurons is a list
        neurons = self.neurons
        if not hasattr(neurons, "__iter__"):
            neurons = [neurons]
        neurons = list(neurons)

        # Make sure self.activations is a list
        activations = self.activations
        if not hasattr(activations, "__iter__"):
            activations = [activations]
        activations = list(activations)

        if np.any(np.array(neurons) <= 0):
            raise ValueError("neurons must be > 0, got %s." %
                             self.neurons)

        if np.any(np.array([activation not in DeepLearningNewsvendor.ACTIVATION_FUNCTIONS.keys() for activation in activations])):
            raise ValueError("Invalid activation function in activations. Supported are %s but got %s"
                             % (list(DeepLearningNewsvendor.ACTIVATION_FUNCTIONS.keys()), activations))

        if len(neurons) != len(activations):
            raise ValueError("Neurons and activations must have same length but neurons is of length %s and "
                             "activations %s " % (len(neurons), len(activations)))

        if self.verbose not in [0, 1, 2]:
            raise ValueError("verbose must be either 0, 1 or 2, got %s." %
                             self.verbose)
                            
        if self.optimizer not in ['adam', 'sgd']:
            raise ValueError("Invalid optimizer. Supported are 'adam' and 'sgd' but got %s" % self.optimizer)


