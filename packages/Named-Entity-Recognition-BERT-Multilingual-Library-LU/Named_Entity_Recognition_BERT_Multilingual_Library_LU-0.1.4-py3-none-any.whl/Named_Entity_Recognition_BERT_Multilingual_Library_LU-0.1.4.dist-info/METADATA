Metadata-Version: 2.1
Name: Named_Entity_Recognition_BERT_Multilingual_Library_LU
Version: 0.1.4
Summary: A comprehensive multilingual Named Entity Recognition (NER) library leveraging BERT. Supports key information extraction tasks across various domains such as biomedical, environmental, and technological.
Home-page: https://github.com/yourusername/Named_Entity_Recognition_BERT_Multilingual_Library_LU
Author: Mehrdad ALMASI, Demival VASQUES FILHO, Tugce KARATAS
Author-email: mehrdad.al.2023@gmail.com, demival.vasques@uni.lu, tugce.karatas@uni.lu
Project-URL: Documentation, https://github.com/yourusername/Named_Entity_Recognition_BERT_Multilingual_Library_LU/wiki
Project-URL: Source, https://github.com/yourusername/Named_Entity_Recognition_BERT_Multilingual_Library_LU
Project-URL: Tracker, https://github.com/yourusername/Named_Entity_Recognition_BERT_Multilingual_Library_LU/issues
Keywords: Named Entity Recognition,BERT,Multilingual NLP,Information Extraction,NER,Key Information Extraction
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing :: Linguistic
Classifier: Programming Language :: Python
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch (>=1.8.0)
Requires-Dist: transformers (>=4.10.0)
Requires-Dist: datasets (>=2.1.0)
Requires-Dist: evaluate (>=0.2.2)
Requires-Dist: numpy (>=1.19.5)
Requires-Dist: seqeval (>=1.2.2)
Provides-Extra: dev
Requires-Dist: pytest (>=6.2) ; extra == 'dev'
Requires-Dist: black ; extra == 'dev'
Requires-Dist: flake8 ; extra == 'dev'
Requires-Dist: isort ; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs ; extra == 'docs'
Requires-Dist: mkdocs-material ; extra == 'docs'

# Named_Entity_Recognition_BERT_Multilingual_Library_LU

## Overview

**Named_Entity_Recognition_BERT_Multilingual_Library_LU** is a powerful and flexible library for Named Entity Recognition (NER) tasks using BERT models. This library supports multilingual NER and is suitable for key information extraction tasks across various domains such as biomedical, environmental, and technological.

The library simplifies NER tasks by providing an easy-to-use pipeline for loading data, training models, and making predictions. It is designed for developers, researchers, and data scientists looking for a robust NER solution.

## Features

- **Multilingual Support**: Leverage the power of BERT for NER tasks in multiple languages.
- **Flexible Input Format**: Works with CoNLL format data.
- **Easy Integration**: Provides a simple `NERPipeline` class for seamless integration into your projects.
- **Comprehensive Metrics**: Evaluate your models with precision, recall, F1-score, and accuracy.
- **Pretrained Models**: Supports any BERT-based pretrained models.

## Installation

Install the library using `pip`:
```bash
pip install named-entity-recognition-bert-multilingual-library-lu
```

## Usage

### Example Usage

Here is a complete example of how to use the library for training and predicting:

```python
from Named_Entity_Recognition_BERT_Multilingual_Library_LU import NERPipeline

# Initialize the pipeline
pipeline = NERPipeline(pretrained_model="bert-base-multilingual-cased")

# Prepare the data
train_dataset, val_dataset, test_dataset = pipeline.prepare_data(
    "path/to/train.tsv", 
    "path/to/val.tsv", 
    "path/to/test.tsv"
)

# Initialize the model with the number of unique labels
num_labels = len(pipeline.label_to_id)
pipeline.initialize_model(num_labels)

# Train the model
pipeline.train(train_dataset, val_dataset)

# Make predictions on a new sentence
test_sentence = "BRCA1 is a gene associated with breast cancer."
predictions = pipeline.predict(test_sentence)

print("Predictions:")
for token, label in predictions:
    print(f"{token}: {label}")
```

## CoNLL Data Format

The input data should be in CoNLL format:

```mathematica

Token1    Label1
Token2    Label2
Token3    Label3
Token4    Label4
```

### Example:

```mathematica
BRCA1       B-GENE
is          O
a           O
gene        O
associated  O
with        O
breast      B-DISEASE
cancer      I-DISEASE
.           O
```

## Key Components

### 1. NERPipeline

The main class providing methods for:

- **Data Preparation**: Converts CoNLL format to a dataset suitable for BERT.
- **Model Initialization**: Loads a pretrained BERT model for NER.
- **Training**: Fine-tunes the model on your data.
- **Prediction**: Predicts labels for new sentences.
- **Evaluation**: Computes evaluation metrics (precision, recall, F1, etc.).

### 2. Helper Functions

The library also includes utility functions for advanced users:

- **`load_data`**: Load and parse CoNLL format data.
- **`convert_to_hf_format`**: Convert data to Hugging Face dataset format.
- **`compute_metrics`**: Evaluate predictions using `seqeval`.

## Evaluation Metrics

The library uses the `seqeval` library to compute the following metrics:

- Precision
- Recall
- F1-Score
- Accuracy

## Dependencies

- `torch`
- `transformers`
- `datasets`
- `evaluate`
- `numpy`
- `seqeval`

## Contributing

We welcome contributions! Please feel free to open issues or submit pull requests.

