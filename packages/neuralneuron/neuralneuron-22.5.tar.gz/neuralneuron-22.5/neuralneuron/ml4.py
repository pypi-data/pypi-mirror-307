def a4():
    print("""# -*- coding: utf-8 -*-
'ML4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7sSI6h1WjzumGvk5fE3WfaQ1mQO-Llk

# Clustering Analysis

A. Implement K-Means clustering on Iris.csv dataset. Determine the number of clusters using the elbow method.
Dataset Link: https://www.kaggle.com/datasets/uciml/iris
'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

iris_data = pd.read_csv('./datasets/Iris.csv')
iris_data.head()

'The Species column contains labels that we won't use for clustering, as K-Means is an unsupervised algorithm.'

# Drop 'Id' and 'Species' columns for clustering
iris_data = iris_data.drop(columns=['Id', 'Species'])

iris_data.isnull().sum()

# Standardize the features
scaler = StandardScaler()
iris_scaled = scaler.fit_transform(iris_data)

# Checking the first few rows of the scaled data
pd.DataFrame(iris_scaled, columns=iris_data.columns).head()

# Apply KMeans clustering for a range of cluster numbers
inertia = []
K = range(1, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(iris_data)
    inertia.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.show()

'**Observation:**

We expect to see a graph where the inertia decreases sharply as we increase the number of clusters but starts leveling off at some point. This "elbow point" is the optimal number of clusters.

### K-Means Clustering with k=3
'

# Apply K-Means with k=3
kmeans_3 = KMeans(n_clusters=3, random_state=42)
iris_data['Cluster_3'] = kmeans_3.fit_predict(iris_scaled)

# Visualizing the clusters using Sepal and Petal length/width
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=iris_data['SepalLengthCm'],
    y=iris_data['PetalLengthCm'],
    hue=iris_data['Cluster_3'],
    palette='Set1',
    s=100
)

plt.title('K-Means Clustering with k=3')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.show()

'## B. Implement K-Mediod Algorithm on a credit card dataset. Determine the number of clusters using the Silhouette Method.
Dataset link: https://www.kaggle.com/datasets/arjunbhasin2013/ccdata
'

!pip install scikit-learn-extra

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn_extra.cluster import KMedoids
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Credit Card dataset
cc_data = pd.read_csv('./datasets/Credit_Card.csv')
cc_data.head()

'**Observation:**

At this point, we should observe the features of the credit card dataset. Typically, it contains variables like Balance, Purchase Frequency, Credit Limit, etc., related to customer spending and financial behavior.

### Step 3: Data Preprocessing
'

# Drop 'CUST_ID' or other non-essential columns if present
cc_data = cc_data.drop(columns=['CUST_ID'], errors='ignore')
cc_data.isnull().sum()

# Fill missing values or remove columns with too many missing entries
cc_data = cc_data.fillna(cc_data.mean())  # Filling missing values with the mean

# Standardize the data
scaler = StandardScaler()
cc_scaled = scaler.fit_transform(cc_data)

# Display the first few rows of scaled data
pd.DataFrame(cc_scaled, columns=cc_data.columns).head()

'### Step 4: K-Medoid Clustering with Silhouette Method for Optimal Number of Clusters'

# Finding the optimal number of clusters using the Silhouette score
silhouette_scores = []
K = range(2, 11)  # Start from 2 clusters since silhouette is undefined for 1 cluster

for k in K:
    kmedoids = KMedoids(n_clusters=k, random_state=42)
    cluster_labels = kmedoids.fit_predict(cc_data)

    # Calculate the silhouette score
    score = silhouette_score(cc_data, cluster_labels)
    silhouette_scores.append(score)

# Plotting the Silhouette scores for different number of clusters
plt.figure(figsize=(8, 5))
plt.plot(K, silhouette_scores, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method for Optimal k')
plt.show()

'**Observation:**

k = 2 provides the highest silhouette score so we select k = 2

### Step 5: Finalizing the K-Medoids Clustering with k=2
'

# Apply K-Medoids clustering with k=2
kmedoids_2 = KMedoids(n_clusters=2, random_state=42)
cc_data['Cluster_2'] = kmedoids_2.fit_predict(cc_data)

# Visualizing the clusters (if you want to visualize two features, you can change x and y)
plt.figure(figsize=(8, 5))
sns.scatterplot(x=cc_data['BALANCE'],
                y=cc_data['PURCHASES'],
                hue=cc_data['Cluster_2'],
                palette='Set1',
                s=100)

plt.title('K-Medoids Clustering with k=2')
plt.xlabel('Balance')
plt.ylabel('Purchases')
plt.show()""")
a4()
