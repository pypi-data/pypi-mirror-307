def b4():
    print("""
# -*- coding: utf-8 -*-
assign4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17MwXTiVsEsQRPgE9lRbdlxlpSGhoHW7w


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score
)
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
# Load and process the dataset
metabric_df = pd.read_csv("C:\\Users\\Dell\\Downloads\\METABRIC_RNA_Mutation.csv")
metabric_df.shape
metabric_df.info(verbose=True)
metabric_df.sample(5)
metabric_df = metabric_df.set_index('patient_id')
df_expression = metabric_df.iloc[:, 30:519].join(metabric_df['overall_survival'], how='inner')
df_expression
# Dictionary to store F1 and accuracy scores of each model
metrics_summary = {"Model": [], "F1 Score": [], "Accuracy": []}

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    model.fit(X_train, y_train)
# Predictions and probabilities (if applicable)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    y_pred = model.predict(X_test)
# Calculate metrics
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
# Store the metrics
    metrics_summary["Model"].append(model_name)
    metrics_summary["F1 Score"].append(f1)
    metrics_summary["Accuracy"].append(accuracy)
# Display classification metrics
    print(f"\n=== {model_name} ===")
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()
# AUC-ROC Score (if applicable)
    if y_proba is not None:
        print(f"AUC-ROC Score: {roc_auc_score(y_test, y_proba):.4f}")
# Main function to process the dataset and evaluate models
def main(df, target_column):
# Prepare the dataset
    X = df.drop(target_column, axis=1)
    y = df[target_column]
# Handle categorical variables
    X = pd.get_dummies(X, drop_first=True)
    if y.dtype == 'object':
        y = LabelEncoder().fit_transform(y)
# Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    models = {
    'Random Forest': RandomForestClassifier(),
    'Support Vector Classifier': SVC(probability=True),
    }
# Train and evaluate all models
    for name, model in models.items():
        evaluate_model(model, X_train, X_test, y_train, y_test, name)
# Plot F1 Scores of all models
    plt.figure(figsize=(10, 5))
    sns.barplot(x=metrics_summary["Model"],
    y=metrics_summary["F1 Score"], palette='coolwarm')
    plt.xticks(rotation=45)
    plt.title('F1 Scores of All Models')
    plt.ylabel('F1 Score')
    plt.show()
# Plot Accuracy Scores of all models
    plt.figure(figsize=(10, 5))
    sns.barplot(x=metrics_summary["Model"],
    y=metrics_summary["Accuracy"], palette='coolwarm')
    plt.xticks(rotation=45)
    plt.title('Accuracy Scores of All Models')
    plt.ylabel('Accuracy')
    plt.show()

if __name__ == "__main__":
# Replace with your dataframe
    df = df_expression # Example input dataframe
    target_column = 'overall_survival' # Replace with the actual target column name
main(df, target_column)


""")
b4()
