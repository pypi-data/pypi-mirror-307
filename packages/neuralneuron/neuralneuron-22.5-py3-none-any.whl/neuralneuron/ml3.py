def a3():
    print("""# -*- coding: utf-8 -*-
'ML3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j2Dl2y2XGtNrLOC2J9mvFa2dRBIw5cub

# **Classification Analysis**

#### A. Implementation of Support Vector Machines (SVM) for classifying images of hand-written digits into their respective numerical classes (0 to 9).

**_Why SVM?_**

Support Vector Machines are powerful for classification problems, especially when dealing with high-dimensional data (like pixel data in images). SVM tries to find the best decision boundary (hyperplane) that separates the classes.
'

from sklearn import datasets
import matplotlib.pyplot as plt

# Load the digits dataset
digits = datasets.load_digits()

# Inspect the dataset
print(f"Dataset shape: {digits.images.shape}")
print(f"Number of samples: {len(digits.images)}")
print(f"Number of features per sample: {digits.data.shape[1]}")
print(f"Target labels (digits): {set(digits.target)}")

# Display a few images with their labels
fig, axes = plt.subplots(1, 5, figsize=(10, 3))
for i, ax in enumerate(axes):
    ax.imshow(digits.images[i], cmap='gray')
    ax.set_title(f"Label: {digits.target[i]}")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images for input to the SVM model
X = digits.data  # Shape: (1797, 64) - each sample has 64 pixel features
y = digits.target  # Shape: (1797,) - target labels (digits 0-9)

# Normalize the pixel values to a range of 0 to 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into 75% training and 25% testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)

# Check the shapes of the train and test sets
print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

'### Choosing the SVM Kernel
The SVM algorithm allows us to use different kernels for mapping non-linear relationships in data:

**Linear Kernel:** Used when the data is linearly separable.

**RBF (Radial Basis Function) Kernel:** Works well for non-linear data by adding a Gaussian transformation to the data points.

We'll experiment with both linear and RBF kernels and see which one performs better on this dataset.

**Why Start with Linear Kernel?**

Since our data has relatively high dimensionality (64 features), linear SVM can be a good first option. If we notice that the linear model is underperforming, we'll try a more flexible non-linear kernel.

---
'

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train, y_train)

y_pred_linear = svm_linear.predict(X_test)

print("\nLinear Kernel SVM Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_linear) * 100:.2f}%")
print(classification_report(y_test, y_pred_linear))

_, axes = plt.subplots(2, 5, figsize=(10, 5))
for ax, image, label in zip(axes.ravel(), X_test, y_test):
    ax.imshow(image.reshape(8, 8), cmap=plt.cm.gray_r)
    ax.set_title(f'True: {label}')
    ax.axis('off')
plt.show()

'### **Conclusion:**

Linear SVM offers a strong, fast baseline performance that is close to 98%, which is excellent given that it operates in a lower-dimensional feature space without mapping to non-linear regions.

These results demonstrate the strength of SVM for image classification, and further improvements could be explored through techniques like feature extraction or dimensionality reduction (e.g., PCA), though the performance is already very strong.

## **B. Implement K-Nearest Neighboursâ€™ algorithm on Social network ad dataset. Compute confusion matrix, accuracy, error rate, precision and recall on the given dataset.**
Dataset link:https://www.kaggle.com/datasets/rakeshrau/social-network-ads

### Step 1: Understanding the Problem

The goal is to use the Social Network Ads dataset to classify whether a user purchases a product based on their age and salary, using the K-Nearest Neighbors (KNN) algorithm. Our evaluation metrics will include the confusion matrix, accuracy, error rate, precision, and recall.

**Why KNN?**

KNN is a simple, non-parametric algorithm that classifies new data points based on similarity (distance) to its neighbors. It's ideal for this classification problem because we have relatively few features (age and salary), and we want to categorize users into one of two classes (Purchased: 0 or 1).

---
'

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

# Load dataset
dataset = pd.read_csv('./datasets/Social_Network_Ads.csv')

# View basic details
print(dataset.info())
print(dataset.head())

'
**Dataset Observations:**

_Features:_

1. `User ID`: Doesn't seem useful for prediction; we can ignore it as it's a unique identifier.
2. `Gender`: Might be interesting, but converting it into numerical form (0 for male, 1 for female) could be useful if we decide to include it.
3. `Age`: Definitely a key factor in predicting purchase behavior.
4. `EstimatedSalary`: Another key factor, as a higher salary could correlate with higher purchasing power.
5. `Purchased`: This is our target variable, where 1 indicates the person made a purchase and 0 indicates they didn't.

'

dataset = dataset.drop(columns=['User ID'])
print(dataset.describe())

dataset['Gender'] = LabelEncoder().fit_transform(dataset['Gender'])

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)
# Check the shapes to ensure proper split
print(f"\nTraining set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# Create KNN classifier with K=5
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test)

'### Step 9: Model Evaluation
We will evaluate the model using various metrics: confusion matrix, accuracy, error rate, precision, and recall.

---
'

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Accuracy, error rate, precision, and recall
accuracy = accuracy_score(y_test, y_pred)
error_rate = 1 - accuracy
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"\nAccuracy: {accuracy * 100:.2f}%")
print(f"Error Rate: {error_rate * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

'---

**Observations:**

- **Confusion Matrix:** Shows the number of True Positives, True Negatives, False Positives, and False Negatives.
- **Accuracy**: The percentage of correct predictions.
- **Error Rate:** The percentage of incorrect predictions.
- **Precision**: How many predicted positives are actually positive.
- **Recall**: How many actual positives were predicted correctly.

---
'

""")
a3()
