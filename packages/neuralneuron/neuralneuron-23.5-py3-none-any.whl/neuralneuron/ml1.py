def a1():
    print("""# -*- coding: utf-8 -*-
'ML1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XSGeauXLE9YCjBxudLHhqIwPfrDiYbLO

## Assignment 1

### Part I

To use PCA Algorithm for dimensionality reduction. You have a dataset that includes measurements for different variables on wine (alcohol, ash, magnesium, and so on). Apply PCA algorithm & transform this data so that most variations in the measurements of the variables are captured by a small number of principal components so that it is easier to distinguish between red and white wine by inspecting these principal components.
'

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

data = pd.read_csv('datasets/Wine.csv')
data.shape

'*   The Significant Predictors can be *Color_Intensity* and *Hue*
*   The Target Variable is ***Customer_Segment***
'

data['Customer_Segment'].unique()

'3 classes/category of wines found'

x = data.iloc[:, :-1]  # Features (all columns except the last)
y = data.iloc[:, -1]   # Target (last column)
xScaled = StandardScaler().fit_transform(x)

'### Principal Component Analysis (PCA)
- Principal Component Analysis (PCA) is a technique used to simplify complex data by finding the most important patterns within it.
- Imagine you have a large dataset with many variables, like a collection of photos with different features (e.g., color, brightness, shape).
- Analyzing all these features at once can be overwhelming and computationally expensive.
- PCA helps by reducing the number of features (or dimensions) while still retaining the most critical information.
- PCA looks at the dataset and finds directions (called "principal components") along which the data varies the most.
'

pca = PCA(n_components=2)
pc = pca.fit_transform(xScaled)

plt.figure(figsize=(8,6))
plt.scatter(pc[:, 0], pc[:, 1], c=y, cmap='viridis', edgecolor='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Wine Dataset')
plt.colorbar(label='Wine Class')
plt.show()

'# Part II

Apply LDA Algorithm on Iris Dataset and classify which species a given flower belongs to
'

from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

iris = pd.read_csv("./datasets/Iris.csv")
iris.drop(columns="Id", inplace=True)

x = iris.iloc[:, :-1]  # Features: sepal length, sepal width, petal length, petal width
y = iris.iloc[:, -1]  # Target: species (0 - Setosa, 1 - Versicolour, 2 - Virginica)

iris.sample(5)

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)

lda = LinearDiscriminantAnalysis()  # Reduce to 2 components for visualization
xtrain = lda.fit_transform(xtrain, ytrain)
xtest = lda.transform(xtest)

lda = LinearDiscriminantAnalysis()
lda.fit(xtrain, ytrain)
ypred = lda.predict(xtest)

'### Linear Discriminant Analysis (LDA)
Linear Discriminant Analysis (LDA) is a technique used to separate data into different classes, making it easier to classify or categorize new data points. It's commonly used in situations where you have labeled data, meaning you know the category each data point belongs to, and you want to maximize the separation between these categories.
'

print("Accuracy:", accuracy_score(ytest, ypred))
print("Confusion Matrix:\n", confusion_matrix(ytest, ypred))
print("Classification Report:\n", classification_report(ytest, ypred, target_names=y.unique()))

sample = pd.DataFrame([[5.1, 3.5]], columns=['PCA1','PCA2'])
predicted_species = lda.predict(sample)
print(f"Predicted Species: {predicted_species[0]}")

plt.figure(figsize=(10, 7))
for species in iris['Species'].unique():
    plt.scatter(xtrain[ytrain == species, 0],
                xtrain[ytrain == species, 1], label=species)

plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('LDA of Iris Dataset')
plt.legend()
plt.show()

""")
a1()
