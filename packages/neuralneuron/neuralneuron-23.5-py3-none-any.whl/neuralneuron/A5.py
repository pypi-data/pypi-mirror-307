def a5():
    print('''

# -*- coding: utf-8 -*-
IR_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ee_xscWgVS-tkLN7uvc38rZZuQkyWhx0


import requests
from bs4 import BeautifulSoup
import csv
import pandas as pd
import time

base_url = 'http://books.toscrape.com/catalogue/category/books_1/page-{}.html'  # Pagination
all_books = []

def crawl_book_site(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36"
    }

    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"Failed to retrieve {url} (status code: {response.status_code})")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')

    books = soup.find_all('article', class_='product_pod')  # Adjusted for Books to Scrape
    book_info = []

    for book in books:
        title = book.h3.a['title']  # Extracting title
        price = book.find('p', class_='price_color').text  # Extracting price
        link = book.h3.a['href']  # Extracting link
        author = "Unknown"  # Books to Scrape does not provide author information directly

        book_info.append({
            'title': title,
            'author': author,
            'price': price,
            'link': link
        })

    return book_info

for i in range(1, 6):  # Adjust the range for more pages
    url = base_url.format(i)
    books = crawl_book_site(url)
    all_books.extend(books)
    time.sleep(1)

with open('books.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['title', 'author', 'price', 'link']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for book in all_books:
        writer.writerow(book)

print("Scraping completed. Data saved to books.csv.")

data=pd.read_csv('books.csv')

print(data.head())


''')
a5()

