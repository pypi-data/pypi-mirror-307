def a5():
    print("""# -*- coding: utf-8 -*-
'ML5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LB2-Oo-KoH4w8yrP5gHeKqYwejFLeq6S

# Ensemble Learning

### A. Implement Random Forest Classifier model to predict the safety of the car.
Dataset link: https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set
'

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score

data = pd.read_csv('./datasets/car_evaluation.csv', header=None)
data.shape

data.info()

'### Step 2: Add headers'

data.columns = ['buying_price', 'maintenance_cost', 'number_of_doors', 'number_of_persons', 'lug_boot', 'safety', 'decision']

data.head()

data.describe()

le = LabelEncoder()
for column in data.columns:
    data[column] = le.fit_transform(data[column])

X = data.drop('decision', axis=1)  # Features
y = data['decision']  # Target (Encoded)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=10000, random_state=42)

rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:')
print(report)

'## **B: Use different voting mechanism and Apply AdaBoost (Adaptive Boosting), Gradient Tree Boosting (GBM), XGBoost classification on Iris dataset and compare the performance of three models using different evaluation measures.**
Dataset Link: https://www.kaggle.com/datasets/uciml/iris

### 1. Import Necessary Libraries
'

# Importing libraries for data manipulation and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for model building and evaluation
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Importing XGBoost
from xgboost import XGBClassifier

# Importing Label Encoder
from sklearn.preprocessing import LabelEncoder

iris_data = pd.read_csv("./datasets/Iris.csv")
iris_data.head()

# Check for missing values
iris_data.isnull().sum()

iris_data['Species'] = LabelEncoder().fit_transform(iris_data['Species'])
iris_data['Species'].unique()

X = iris_data.drop('Species', axis=1)
y = iris_data['Species']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

'- **Stratification**: We use `stratify=y` to ensure that the proportion of classes in both training and test sets is the same as in the original dataset. This prevents bias, especially if the dataset is imbalanced.

#### AdaBoost,Gradient Boosting and XGBoost Classifiers
'

# Initialize and train AdaBoost Classifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)

ada_model.fit(X_train, y_train)
gb_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Predict on the test data
y_pred_ada = ada_model.predict(X_test)
y_pred_gb = gb_model.predict(X_test)
y_pred_xgb = xgb_model.predict(X_test)

'---

#### Observations:
- **AdaBoost**: It sequentially adjusts the weights of misclassified instances, focusing on hard-to-classify examples.
- **Gradient Boosting**: Focuses on minimizing the errors of the previous trees by using residuals, making it a more refined boosting method compared to AdaBoost.
- **XGBoost**: It optimizes Gradient Boosting by using regularization to avoid overfitting, making it a more powerful and faster method.
- All three classifiers are trained on the training set using 100 estimators.

---

### 7. Evaluate the Models

We will evaluate the performance of each model using common classification metrics like **accuracy**, **confusion matrix**, and **classification report** (which includes precision, recall, and F1-score).
'

def evaluate_model(y_true, y_pred):
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("\nClassification Report:\n", classification_report(y_true, y_pred))
    print(f'Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\n')


print("AdaBoost Performance:")
evaluate_model(y_test, y_pred_ada)

print("\n\n\n\nGradient Boosting Performance:")
evaluate_model(y_test, y_pred_gb)

print("\n\n\n\nXGBoost Performance:")
evaluate_model(y_test, y_pred_xgb)

'#### Observations:
- We expect **XGBoost** to have the highest accuracy due to its optimizations and regularization techniques, but **Gradient Boosting** and **AdaBoost** are also strong contenders.
- Each model has its own strengths and trade-offs. AdaBoost is simpler and faster for smaller datasets, Gradient Boosting is more powerful for structured data, and XGBoost tends to be the most accurate for larger, more complex datasets.
- **Based on the results**, we would choose the model that best fits the problem's needs (accuracy, computational efficiency, or the ability to handle large datasets).

### Conclusion:

1. We imported and preprocessed the **Iris dataset**, splitting it into training and testing sets.
2. We trained three different boosting algorithms: **AdaBoost**, **Gradient Boosting**, and **XGBoost**.
3. We evaluated the models on accuracy, classification reports, and confusion matrices.
4. We compared the results of each model, and based on observations, **XGBoost** is likely to be the best choice for this dataset.
'

""")
a5()
