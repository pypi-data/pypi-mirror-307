[model]
context_len = 168
pred_len = 24
vocab_size = 3747
num_encoder_layers = 2
num_decoder_layers = 2
d_model = 256
nhead = 4
dim_feedforward = 512
dropout = 0.0
activation = 'gelu'
continuous_loads = false
ignore_spatial = false

[pretrain]
batch_size = 256
init_scale = 0.02
warmup_steps = 10000
lr = 0.0006
train_tokens = 1000000000
tokenizer_without_merge = false

[zero_shot]
tokenizer_without_merge = false

[transfer_learning]
tokenizer_without_merge = false
