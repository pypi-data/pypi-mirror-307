{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing Pipeline Setup\n",
    "--------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "\n",
    "The first thing we need to be able to do is access the data files and interact with them (e.g., read the metadata)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n",
    "\n",
    "The convention with notebooks (and python in general) is to import the nescessary packages as the first thing.\n",
    "\n",
    "\n",
    "We are using `napari` for visualization, and `scipy` `ndimage` and `skimage` for analyzing the image files.  The underlying data format are `numpy` `ndarrays` and tools from  Allen Institute for Cell Science.\n",
    "\n",
    "> #### NOTES:\n",
    "> There are a few convences used here worth explanation.  Note the `imports.py` and `constants.py` files in the base level of the `infer_subc` module.  These provide sortcuts for keeping track of imports and constants.   cf. the bottom of the imports below.  A second thing to note is the use of the \"magics\" ([[ link to magics info %%]]) `%load_ext autoreload` `%autoreload 2`, which tells the notebook to reload any changes made in the source code of the module on change; hence, avoid re-executing the imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from typing import Union, List, Tuple, Any\n",
    "# TODO:  prune the imports.. this is the big set for almost all organelles\n",
    "# # function for core algorithm\n",
    "from scipy import ndimage as ndi\n",
    "import aicssegmentation\n",
    "from aicssegmentation.core.pre_processing_utils import ( intensity_normalization, \n",
    "                                                         image_smoothing_gaussian_slice_by_slice )\n",
    "\n",
    "# # package for io \n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "import napari\n",
    "\n",
    "### import local python functions in ../infer_subc\n",
    "sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "\n",
    "from infer_subc.core.file_io import (read_czi_image,\n",
    "                                                                    list_image_files)\n",
    "from infer_subc.core.img import *\n",
    "from infer_subc.constants import (\n",
    "    TEST_IMG_N,\n",
    "    NUC_CH ,\n",
    "    LYSO_CH ,\n",
    "    MITO_CH ,\n",
    "    GOLGI_CH ,\n",
    "    PEROX_CH ,\n",
    "    ER_CH ,\n",
    "    LD_CH ,\n",
    "    RESIDUAL_CH , \n",
    "    ALL_CHANNELS)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and load Image for processing\n",
    "Read the image and metadata into memory as an ndarray and dictionary from the `.czi` or '.tiff' files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use one file as a test file. Here we are using the TEST_IMG_N constant to determine which image in the list we will use. \n",
    "test_img_n = TEST_IMG_N\n",
    "\n",
    "# Define the path to the directory that contains the input image folder.\n",
    "data_root_path = Path(os.path.expanduser(\"~\")) / \"Documents/Python_Scripts/Infer-subc\"\n",
    "\n",
    "# Specify which subfolder that contains the input data and the input data file extension\n",
    "in_data_path = data_root_path / \"raw\"\n",
    "im_type = \".czi\"\n",
    "\n",
    "# Create a list of the file paths for each image in the input folder. Select test image path.\n",
    "img_file_list = list_image_files(in_data_path,im_type)\n",
    "test_img_name = img_file_list[test_img_n]\n",
    "\n",
    "# Create the output directory to save the segmentation outputs in.\n",
    "out_data_path = data_root_path / \"out\"\n",
    "\n",
    "if not Path.exists(out_data_path):\n",
    "    Path.mkdir(out_data_path)\n",
    "    print(f\"making {out_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image and metadata as an ndarray and dictionary from the test image selected above. \n",
    "img_data,meta_dict = read_czi_image(test_img_name)\n",
    "\n",
    "# Define some of the metadata features.\n",
    "channel_names = meta_dict['name']\n",
    "img = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## SUMMARY\n",
    "\n",
    "The above shows the general procedure for importing the relavent modules, setting up the file I/O, and reading in the multi channel 3D flourescence image.\n",
    "\n",
    "### NEXT:  SEGMENT NUCLEI\n",
    "\n",
    "proceed to [01_infer_nuclei.ipynb](./01_infer_nuclei.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6148ef1fb015fb20f0b6da2ea61c87c6b848bdf3dabb03087e5d5cd0c4607e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
