from _typeshed import Incomplete
from awsglue.data_sink import DataSink as DataSink
from awsglue.data_source import DataSource as DataSource
from awsglue.dataframereader import DataFrameReader as DataFrameReader
from awsglue.dataframewriter import DataFrameWriter as DataFrameWriter
from awsglue.dynamicframe import (
    DynamicFrame as DynamicFrame,
    DynamicFrameCollection as DynamicFrameCollection,
    DynamicFrameReader as DynamicFrameReader,
    DynamicFrameWriter as DynamicFrameWriter,
)
from awsglue.gluetypes import DataType as DataType
from awsglue.streaming_data_source import StreamingDataSource as StreamingDataSource
from awsglue.utils import callsite as callsite, makeOptions as makeOptions
from py4j.java_gateway import JavaClass as JavaClass
from pyspark.sql import SQLContext
from pyspark.sql.utils import StreamingQueryException as StreamingQueryException

def register(sc) -> None: ...

class GlueContext(SQLContext):
    Spark_SQL_Formats: Incomplete
    Unsupported_Compression_Types: Incomplete
    create_dynamic_frame: Incomplete
    create_data_frame: Incomplete
    write_dynamic_frame: Incomplete
    write_data_frame: Incomplete
    spark_session: Incomplete
    def __init__(self, sparkContext, **options) -> None: ...
    def getSource(self, connection_type, format: Incomplete | None = None, transformation_ctx: str = '', push_down_predicate: str = '', **options): ...
    def getStreamingSource(self, connection_type, format: Incomplete | None = None, transformation_ctx: str = '', push_down_predicate: str = '', **options): ...
    def get_catalog_schema_as_spark_schema(self, database: Incomplete | None = None, table_name: Incomplete | None = None, catalog_id: Incomplete | None = None): ...
    def create_dynamic_frame_from_rdd(self, data, name, schema: Incomplete | None = None, sample_ratio: Incomplete | None = None, transformation_ctx: str = ''): ...
    def create_dynamic_frame_from_catalog(self, database: Incomplete | None = None, table_name: Incomplete | None = None, redshift_tmp_dir: str = '', transformation_ctx: str = '', push_down_predicate: str = '', additional_options={}, catalog_id: Incomplete | None = None, **kwargs): ...
    def create_data_frame_from_catalog(self, database: Incomplete | None = None, table_name: Incomplete | None = None, redshift_tmp_dir: str = '', transformation_ctx: str = '', push_down_predicate: str = '', additional_options={}, catalog_id: Incomplete | None = None, **kwargs): ...
    def create_dynamic_frame_from_options(self, connection_type, connection_options={}, format: Incomplete | None = None, format_options={}, transformation_ctx: str = '', push_down_predicate: str = '', **kwargs): ...
    def create_sample_dynamic_frame_from_catalog(self, database: Incomplete | None = None, table_name: Incomplete | None = None, num: Incomplete | None = None, sample_options={}, redshift_tmp_dir: str = '', transformation_ctx: str = '', push_down_predicate: str = '', additional_options={}, catalog_id: Incomplete | None = None, erieTxId: str = '', asOfTime: str = '', **kwargs): ...
    def create_sample_dynamic_frame_from_options(self, connection_type, connection_options={}, num: Incomplete | None = None, sample_options={}, format: Incomplete | None = None, format_options={}, transformation_ctx: str = '', push_down_predicate: str = '', **kwargs): ...
    def create_data_frame_from_options(self, connection_type, connection_options={}, format: Incomplete | None = None, format_options={}, transformation_ctx: str = '', push_down_predicate: str = '', **kwargs): ...
    def getSink(self, connection_type, format: Incomplete | None = None, transformation_ctx: str = '', **options): ...
    def write_dynamic_frame_from_options(self, frame, connection_type, connection_options={}, format: Incomplete | None = None, format_options={}, transformation_ctx: str = ''): ...
    def write_from_options(self, frame_or_dfc, connection_type, connection_options={}, format={}, format_options={}, transformation_ctx: str = '', **kwargs): ...
    def write_dynamic_frame_from_catalog(self, frame, database: Incomplete | None = None, table_name: Incomplete | None = None, redshift_tmp_dir: str = '', transformation_ctx: str = '', additional_options={}, catalog_id: Incomplete | None = None, **kwargs): ...
    def write_data_frame_from_catalog(self, frame, database: Incomplete | None = None, table_name: Incomplete | None = None, redshift_tmp_dir: str = '', transformation_ctx: str = '', additional_options={}, catalog_id: Incomplete | None = None, **kwargs): ...
    def write_dynamic_frame_from_jdbc_conf(self, frame, catalog_connection, connection_options={}, redshift_tmp_dir: str = '', transformation_ctx: str = '', catalog_id: Incomplete | None = None) -> None: ...
    def write_from_jdbc_conf(self, frame_or_dfc, catalog_connection, connection_options={}, redshift_tmp_dir: str = '', transformation_ctx: str = '', catalog_id: Incomplete | None = None): ...
    def convert_resolve_option(self, path, action, target): ...
    def extract_jdbc_conf(self, connection_name, catalog_id: Incomplete | None = None): ...
    def purge_table(self, database, table_name, options={}, transformation_ctx: str = '', catalog_id: Incomplete | None = None) -> None: ...
    def purge_s3_path(self, s3_path, options={}, transformation_ctx: str = '') -> None: ...
    def transition_table(self, database, table_name, transition_to, options={}, transformation_ctx: str = '', catalog_id: Incomplete | None = None) -> None: ...
    def transition_s3_path(self, s3_path, transition_to, options={}, transformation_ctx: str = '') -> None: ...
    def get_logger(self): ...
    def currentTimeMillis(self): ...
    def getSampleStreamingDynamicFrame(self, frame, options={}, batch_function: Incomplete | None = None): ...
    def forEachBatch(self, frame, batch_function, options={}) -> None: ...
    def add_ingestion_time_columns(self, frame, time_granularity): ...
    def start_transaction(self, read_only): ...
    def commit_transaction(self, transaction_id, wait_for_commit: bool = True): ...
    def cancel_transaction(self, transaction_id): ...
